{
  "documents": {
    "psyn-eng_item001": {
      "document_metadata": {
        "language_family": "eng",
        "language_variant": "usa",
        "title": "Examining the mental health symptoms of neurodivergent individuals across demographic and identity factors: a quantitative analysis",
        "authors": [
          {
            "name": "Elizabeth Kroll",
            "affiliation": "Charlie Health, Inc., Bozeman, MT, United States"
          },
          {
            "name": "Megan Lederman",
            "affiliation": "Charlie Health, Inc., Bozeman, MT, United States"
          },
          {
            "name": "Jonathan Kohlmeier",
            "affiliation": "Charlie Health, Inc., Bozeman, MT, United States"
          },
          {
            "name": "Jaime Ballard",
            "affiliation": "The Center for Applied Research and Educational Improvement, University of Minnesota, Saint Paul, MN, United States"
          },
          {
            "name": "Izabella Zant",
            "affiliation": "Charlie Health, Inc., Bozeman, MT, United States"
          },
          {
            "name": "Caroline Fenkel",
            "affiliation": "Charlie Health, Inc., Bozeman, MT, United States"
          }
        ],
        "publisher": "Frontiers in Psychology",
        "publication_year": "2025",
        "domain": "psychology_neurodivergence",
        "text_type": "Original research",
        "purpose": "To analyze how neurodivergent identities interact with other demographic and identity factors to influence mental health symptoms and treatment outcomes.",
        "point_of_view": "Empirical and identity-affirming perspective focused on intersectionality and inclusivity in mental health treatment.",
        "audience": "Mental health professionals, clinicians, researchers, and policy makers interested in neurodiversity and intersectionality.",
        "reach": "International, with a focus on U.S. virtual mental health treatment populations.",
        "topics": [
          "Neurodivergence and mental health",
          "Intersectionality in clinical outcomes",
          "Quantitative analysis of identity factors",
          "Inclusive mental health treatment models"
        ],
        "summary": "This study explores how neurodivergent identities interact with race, gender, and sexual orientation to impact mental health symptoms like depression and anxiety. Using data from over 14,000 clients in a virtual mental health program, the study found significant interaction effects—particularly with gender and sexuality—but not with race. It highlights the importance of identity-affirming care to improve treatment equity and outcomes.",
        "suggested_citation": "Kroll E, Lederman M, Kohlmeier J, Ballard J, Zant I, Fenkel C (2025). Examining the mental health symptoms of neurodivergent individuals across demographic and identity factors: a quantitative analysis. Front. Psychol. 16:1499390. doi:10.3389/fpsyg.2025.1499390"
      },
      "processing_metadata": {
        "submission_file_name": "Examining the mental health symptoms of neurodivergent individuals across demographic and identity factors a quantitative analysis.pdf",
        "creation_date": "2025-03-19",
        "word_count": 8725,
        "status": "vectorization_pending",
        "file_paths": {
          "original": "psyn/eng/submissions/Examining the mental health symptoms of neurodivergent individuals across demographic and identity factors a quantitative analysis.pdf",
          "processed": "psyn/eng/processed/psyn-eng_item001.json"
        },
        "processing_notes": "Data collected from 14,219 individuals in a virtual outpatient program. Relevant for intersectionality and neurodivergence research corpus."
      }
    },
    "gai-eng_item002": {
      "document_metadata": {
        "language_family": "eng",
        "language_variant": "usa",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
        "authors": [
          {
            "name": "Emily M. Bender",
            "affiliation": "University of Washington"
          },
          {
            "name": "Timnit Gebru",
            "affiliation": "Black in AI"
          },
          {
            "name": "Angelina McMillan-Major",
            "affiliation": "University of Washington"
          },
          {
            "name": "Shmargaret Shmitchell",
            "affiliation": "The Aether"
          }
        ],
        "publisher": "Conference on Fairness, Accountability, and Transparency (FAccT '21)",
        "publication_year": "2021",
        "domain": "generative_ai",
        "text_type": "Academic paper",
        "purpose": "Informational",
        "point_of_view": "Academic",
        "audience": "Researchers",
        "reach": "Global",
        "topics": [
          "language models",
          "AI ethics",
          "environmental impact",
          "bias",
          "NLP",
          "machine learning"
        ],
        "summary": "This paper critically examines the trend toward ever-larger language models in NLP, questioning whether “bigger is always better.” The authors identify four major areas of concern: (1) Environmental and financial costs - training large models produces substantial CO2 emissions and creates barriers to entry; (2) Unfathomable training data - massive internet-scraped datasets encode hegemonic viewpoints and biases that harm marginalized communities; (3) Misdirected research effort - focus on benchmark performance may not lead to genuine language understanding; and (4) Risk of harm - these models can generate seemingly coherent but biased or harmful text, which the authors term “stochastic parrots.” The paper argues that large language models manipulate linguistic form without true understanding, yet their fluent output can mislead users into attributing meaning and intent where none exists. This creates risks including reinforcement of stereotypes, generation of abusive content, and potential misuse by bad actors. The authors recommend shifting toward more careful data curation, stakeholder-centered design approaches, and research directions that prioritize understanding over scale, while considering the environmental and social costs of increasingly large models.",
        "suggested_citation": "Bender, E.M., Gebru, T., McMillan-Major, A. and Shmitchell, S. (2021) 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?', Conference on Fairness, Accountability, and Transparency (FAccT '21)."
      },
      "processing_metadata": {
        "submission_file_name": "Bender_DangersOfStochasticParrots_2021.pdf",
        "creation_date": "2024-05-19",
        "word_count": 15544,
        "status": "vectorization_pending",
        "file_paths": {
          "original": "gai/eng/submissions/Bender_DangersOfStochasticParrots_2021.pdf",
          "processed": "gai/eng/processed/gai-eng_item002.json"
        },
        "processing_notes": "Have not yet determined a way to maintain italicized and bolded text in JSON files."
      }
    },
    "gai-eng_item003": {
      "document_metadata": {
        "language_family": "eng",
        "language_variant": "eur",
        "title": "Recommendation on the Ethics of Artificial Intelligence",
        "authors": [
          {
            "name": "UNESCO",
            "affiliation": "United Nations Educational, Scientific and Cultural Organization"
          }
        ],
        "publisher": "UNESCO",
        "publication_year": "2022",
        "domain": "generative_ai",
        "text_type": "Policy document",
        "purpose": "Informational",
        "point_of_view": "Regulatory body",
        "audience": "Policy makers",
        "reach": "Global",
        "topics": [
          "AI ethics",
          "responsible AI",
          "policy framework",
          "human rights",
          "governance",
          "data protection",
          "sustainability",
          "international cooperation"
        ],
        "summary": "This document contains the UNESCO Recommendation on the Ethics of Artificial Intelligence, adopted on November 23, 2021. It provides the first global standard-setting instrument on AI ethics, establishing a comprehensive framework of values, principles, and policy actions to guide the ethical development and deployment of AI systems throughout their lifecycle. The recommendation addresses AI ethics across UNESCO's core domains: education, science, culture, and communication/information. It emphasizes human rights, environmental protection, gender equality, and inclusive development, with particular attention to the needs of developing countries. The framework includes four foundational values (human rights and dignity, environmental flourishing, diversity and inclusiveness, and peaceful interconnected societies) and ten guiding principles covering areas such as proportionality, fairness, transparency, accountability, and human oversight. The document outlines eleven policy areas for implementation, including ethical impact assessment, governance frameworks, data policy, international cooperation, and sector-specific guidance for education, health, economy, and culture. It establishes mechanisms for monitoring and evaluation, and emphasizes the need for multi-stakeholder collaboration to ensure AI technologies serve humanity's benefit while preventing harm to individuals, societies, and ecosystems.",
        "suggested_citation": "UNESCO (2022) Recommendation on the Ethics of Artificial Intelligence. UNESCO."
      },
      "processing_metadata": {
        "submission_file_name": "UNESCO_EthicsInAI_2022.pdf",
        "creation_date": "2024-05-20",
        "word_count": 14571,
        "status": "vectorization_pending",
        "file_paths": {
          "original": "gai/eng/submissions/UNESCO_EthicsInAI_2022.pdf",
          "processed": "gai/eng/processed/gai-eng_item003.json"
        },
        "processing_notes": "The JSON captures the main contents but omits the copyright content on page two and the content from the back cover."
        }
    }
  }  
}